{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d511a401-81c8-4237-b32f-0004d396f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1525948-a0e6-4379-8175-29cf2479bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all modules.\n"
     ]
    }
   ],
   "source": [
    "from model import MemoryRNN\n",
    "from data_loader import generate_task_data\n",
    "from train import train_model\n",
    "print(\"Successfully imported all modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f02153c-899e-4e76-bc9e-90d5326e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CONFIGURATION: Defining the Experiment Parameters ---\n",
    "\n",
    "INPUT_SIZE = 3      # Vocabulary size (Pad, A, B)\n",
    "HIDDEN_SIZE = 16    # The model's memory capacity\n",
    "OUTPUT_SIZE = 3     # Number of possible answers the model can give\n",
    "SEQUENCE_LENGTH = 10# How long the model needs to remember the signal\n",
    "#250  600  900 1300\n",
    "NUM_EPOCHS = 1300   # How many training cycles to run\n",
    "BATCH_SIZE = 128    # How many examples to show the model at once\n",
    "LEARNING_RATE = 0.005 # How quickly the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98f022e-9a1b-4d7a-8270-6bde0c72078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the MemoryRNN model...\n",
      "MemoryRNN(\n",
      "  (rnn): RNN(3, 16, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- 3. INSTANTIATION: Building the Model from Our Blueprint ---\n",
    "# We create an actual instance of our model using the blueprint from model.py\n",
    "# and the parameters we defined above.\n",
    "print(\"\\nCreating the MemoryRNN model...\")\n",
    "rnn_model = MemoryRNN(\n",
    "    input_size=INPUT_SIZE, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    output_size=OUTPUT_SIZE\n",
    ")\n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4dbcab8-276a-4da2-a9de-aebead999c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "Epoch [25/1300], Loss: 0.7320\n",
      "Epoch [50/1300], Loss: 0.7002\n",
      "Epoch [75/1300], Loss: 0.6974\n",
      "Epoch [100/1300], Loss: 0.7146\n",
      "Epoch [125/1300], Loss: 0.7046\n",
      "Epoch [150/1300], Loss: 0.6963\n",
      "Epoch [175/1300], Loss: 0.6952\n",
      "Epoch [200/1300], Loss: 0.6922\n",
      "Epoch [225/1300], Loss: 0.7018\n",
      "Epoch [250/1300], Loss: 0.6853\n",
      "Epoch [275/1300], Loss: 0.6941\n",
      "Epoch [300/1300], Loss: 0.6939\n",
      "Epoch [325/1300], Loss: 0.6936\n",
      "Epoch [350/1300], Loss: 0.6929\n",
      "Epoch [375/1300], Loss: 0.6933\n",
      "Epoch [400/1300], Loss: 0.6895\n",
      "Epoch [425/1300], Loss: 0.6937\n",
      "Epoch [450/1300], Loss: 0.6940\n",
      "Epoch [475/1300], Loss: 0.6932\n",
      "Epoch [500/1300], Loss: 0.6926\n",
      "Epoch [525/1300], Loss: 0.7014\n",
      "Epoch [550/1300], Loss: 0.6875\n",
      "Epoch [575/1300], Loss: 0.6962\n",
      "Epoch [600/1300], Loss: 0.6932\n",
      "Epoch [625/1300], Loss: 0.6930\n",
      "Epoch [650/1300], Loss: 0.6934\n",
      "Epoch [675/1300], Loss: 0.6905\n",
      "Epoch [700/1300], Loss: 0.6919\n",
      "Epoch [725/1300], Loss: 0.6930\n",
      "Epoch [750/1300], Loss: 0.6937\n",
      "Epoch [775/1300], Loss: 0.6931\n",
      "Epoch [800/1300], Loss: 0.7027\n",
      "Epoch [825/1300], Loss: 0.6912\n",
      "Epoch [850/1300], Loss: 0.6890\n",
      "Epoch [875/1300], Loss: 0.6947\n",
      "Epoch [900/1300], Loss: 0.6906\n",
      "Epoch [925/1300], Loss: 0.6878\n",
      "Epoch [950/1300], Loss: 0.6964\n",
      "Epoch [975/1300], Loss: 0.6911\n",
      "Epoch [1000/1300], Loss: 0.7009\n",
      "Epoch [1025/1300], Loss: 0.6950\n",
      "Epoch [1050/1300], Loss: 0.6998\n",
      "Epoch [1075/1300], Loss: 0.6939\n",
      "Epoch [1100/1300], Loss: 0.6956\n",
      "Epoch [1125/1300], Loss: 0.6935\n",
      "Epoch [1150/1300], Loss: 0.7093\n",
      "Epoch [1175/1300], Loss: 0.6922\n",
      "Epoch [1200/1300], Loss: 0.7022\n",
      "Epoch [1225/1300], Loss: 0.6954\n",
      "Epoch [1250/1300], Loss: 0.6952\n",
      "Epoch [1275/1300], Loss: 0.6945\n",
      "Epoch [1300/1300], Loss: 0.6934\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Testing the Trained Model on a New, Unseen Example ---\n",
      "The task was to remember the signal: 'A'\n",
      "The model's final prediction was:     'A'\n",
      "\n",
      "[SUCCESS]: The model has learned to integrate information over time.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. EXECUTION: Running the Training Process ---\n",
    "# We call our training function from train.py, passing it the model we just\n",
    "# created and all the training parameters. This function will return the\n",
    "# model after its weights have been updated through learning.\n",
    "trained_model = train_model(\n",
    "    model=rnn_model,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# --- 5. EXAMINATION: Testing the Trained Model ---\n",
    "# This is the most important part: the final exam for our model.\n",
    "# We must verify that it has actually learned the task.\n",
    "\n",
    "print(\"\\n--- Testing the Trained Model on a New, Unseen Example ---\")\n",
    "\n",
    "# Set the model to evaluation mode. This is a good practice that turns off\n",
    "# certain training-specific layers like Dropout.\n",
    "trained_model.eval()\n",
    "\n",
    "# The `torch.no_grad()` context manager tells PyTorch that we are not\n",
    "# training, so it doesn't need to calculate gradients, which saves memory and computation.\n",
    "with torch.no_grad():\n",
    "    # Generate one single, new test sample the model has never seen before.\n",
    "    test_input, test_label = generate_task_data(1, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Get the model's raw output (logits) for this test sample.\n",
    "    test_output = trained_model(test_input)\n",
    "    \n",
    "    # Find the model's actual prediction by finding the index of the highest logit.\n",
    "    _, predicted_idx = torch.max(test_output.data, 1)\n",
    "\n",
    "    signal_map = {1: 'A', 2: 'B'}\n",
    "    correct_signal = signal_map[test_label.item()]\n",
    "    predicted_signal = signal_map[predicted_idx.item()]\n",
    "    \n",
    "    print(f\"The task was to remember the signal: '{correct_signal}'\")\n",
    "    print(f\"The model's final prediction was:     '{predicted_signal}'\")\n",
    "    \n",
    "    if correct_signal == predicted_signal:\n",
    "        print(\"\\n[SUCCESS]: The model has learned to integrate information over time.\")\n",
    "    else:\n",
    "        print(\"\\n[FAILURE]: The model did not learn the task. Consider increasing NUM_EPOCHS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fc668-c069-4b2a-b58f-eb1155fa1a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
